{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Red neuronal convolucional para el diagnóstico de nódulos tiroideos según la clasificación EU-TIRADS**\n",
    "\n",
    "## Por Alejandro Martínez Hernández\n",
    "\n",
    "### Notebook 3/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creación de modelos**\n",
    "\n",
    "PAra la siguiente parte se crearan los siguientes modelos de clasificación simple:\n",
    "- SVM con crossvalidation y gridsearch para buscar parámetros.\n",
    "- SVM con preprocesamiento de datos con PCA, crossvalidation y gridsearch para buscar parámetros.\n",
    "- Forest con crossvalidation y gridsearch para buscar parámetros.\n",
    "- Forest con preprocesamiento de datos con PCA, crossvalidation y gridsearch para buscar parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # valores de C\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.001],  # valores de gamma\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # tipos de kernel\n",
    "}\n",
    "\n",
    "# Crear el modelo SVM\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**\n",
    "\n",
    "En este código, se han modificado varias partes del anterior para incluir PCA:\n",
    "\n",
    "- **Pipeline de sklearn:** Utiliza Pipeline para encadenar PCA y SVM. Esto garantiza que PCA se aplique a los datos antes de que se pase a SVM en cada iteración del proceso de entrenamiento y validación cruzada.\n",
    "\n",
    "- **Parámetros de GridSearch:** Se actualizó param_grid para ajustarlo al pipeline, especificando que los parámetros se apliquen al estimador SVM dentro del pipeline (notado por el prefijo 'svm__' en los nombres de los parámetros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Definir el número de componentes para PCA\n",
    "n_components = 50  # Este valor puede ser ajustado según la varianza a conservar\n",
    "\n",
    "# Crear un pipeline que incluya PCA y SVM\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100],  # Número de árboles en el bosque\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Número de características a considerar al buscar la mejor división\n",
    "    'max_depth': [None, 10, 20],  # Profundidad máxima del árbol\n",
    "    'min_samples_split': [2, 5],  # Número mínimo de muestras requeridas para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2],  # Número mínimo de muestras requeridas en cada hoja del árbol\n",
    "    'bootstrap': [True, False]  # Método para muestrear puntos de datos (con o sin reemplazo)\n",
    "}\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "forest_model = RandomForestClassifier()\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(forest_model, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Definir el número de componentes principales para PCA\n",
    "n_components = 50\n",
    "\n",
    "# Crear un pipeline que incluya PCA y Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('forest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'forest__n_estimators': [10, 100],\n",
    "    'forest__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'forest__max_depth': [None, 10, 20],\n",
    "    'forest__min_samples_split': [2, 5],\n",
    "    'forest__min_samples_leaf': [1, 2],\n",
    "    'forest__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
