{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Red neuronal convolucional para el diagnóstico de nódulos tiroideos según la clasificación EU-TIRADS**\n",
    "\n",
    "## Por Alejandro Martínez Hernández\n",
    "\n",
    "### Notebook 3/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creación de modelos**\n",
    "\n",
    "PAra la siguiente parte se crearan los siguientes modelos de clasificación simple:\n",
    "- SVM con crossvalidation y gridsearch para buscar parámetros.\n",
    "- SVM con preprocesamiento de datos con PCA, crossvalidation y gridsearch para buscar parámetros.\n",
    "- Forest con crossvalidation y gridsearch para buscar parámetros.\n",
    "- Forest con preprocesamiento de datos con PCA, crossvalidation y gridsearch para buscar parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # valores de C\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.001],  # valores de gamma\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']  # tipos de kernel\n",
    "}\n",
    "\n",
    "# Crear el modelo SVM\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**\n",
    "\n",
    "En este código, se han modificado varias partes del anterior para incluir PCA:\n",
    "\n",
    "- **Pipeline de sklearn:** Utiliza Pipeline para encadenar PCA y SVM. Esto garantiza que PCA se aplique a los datos antes de que se pase a SVM en cada iteración del proceso de entrenamiento y validación cruzada.\n",
    "\n",
    "- **Parámetros de GridSearch:** Se actualizó param_grid para ajustarlo al pipeline, especificando que los parámetros se apliquen al estimador SVM dentro del pipeline (notado por el prefijo 'svm__' en los nombres de los parámetros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Número de Componentes para PCA**\n",
    "Este será un método que se aplicará en cada uno de los modelos para explorar si mejoran o no la selección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Función para cargar y procesar imágenes\n",
    "def load_and_preprocess_images(data_dir, categories, image_size=(128, 128)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(data_dir, category)\n",
    "        for img in os.listdir(path):\n",
    "            img_path = os.path.join(path, img)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is not None:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                image = cv2.resize(image, image_size)\n",
    "                flattened_image = image.flatten()\n",
    "                data.append(flattened_image)\n",
    "                labels.append(category)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Cargar y procesar las imágenes\n",
    "data, labels = load_and_preprocess_images(data_dir, categories)\n",
    "\n",
    "# Realizar PCA para determinar el número de componentes principales\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "# Graficar porcentaje de varianza acumulada vs número de componentes principales\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Número de componentes principales vs Porcentaje de varianza acumulada')\n",
    "plt.xlabel('Número de componentes principales')\n",
    "plt.ylabel('Porcentaje de varianza acumulada')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se conoce que no hay un número \"correcto\" universal para el número de componentes; depende del equilibrio entre la simplicidad del modelo y la cantidad de varianza explicada. Para los siguientes modelos se tomara un número de componentes principales igual a 50 con lso que se pueden explicar poco más del 70% de la varianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Definir el número de componentes para PCA\n",
    "n_components = 50  # Este valor puede ser ajustado según la varianza a conservar\n",
    "\n",
    "# Crear un pipeline que incluya PCA y SVM\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('svm', svm.SVC())\n",
    "])\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.001],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "240 fits failed out of a total of 720.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.60682624 0.65274823 0.5858156  0.63572695\n",
      " 0.56923759 0.6820922  0.56897163 0.63182624 0.59024823 0.61923759\n",
      " 0.5608156  0.69033688 0.60265957 0.6358156  0.59423759 0.64441489\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.61046099 0.68634752 0.59804965 0.66968085\n",
      " 0.58138298 0.66099291 0.58147163 0.64840426 0.66542553 0.64840426\n",
      " 0.58546099 0.65700355 0.5858156  0.61533688 0.61046099 0.67358156\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.61117021 0.66950355 0.60248227 0.63590426\n",
      " 0.5731383  0.63617021 0.58111702 0.63608156 0.54406028 0.66968085\n",
      " 0.61941489 0.65682624 0.58156028 0.59822695 0.61498227 0.64840426\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.58528369 0.64423759 0.70265957 0.68200355\n",
      " 0.61932624 0.64840426 0.57721631 0.68634752 0.64051418 0.66515957\n",
      " 0.65274823 0.67340426 0.56879433 0.65682624 0.64849291 0.66108156\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.5606383  0.63625887 0.58111702 0.62757092\n",
      " 0.61914894 0.65700355 0.64849291 0.64867021 0.61870567 0.63590426\n",
      " 0.65248227 0.63998227 0.59432624 0.67774823 0.59796099 0.66099291\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.59370567 0.64406028 0.62730496 0.66090426\n",
      " 0.59397163 0.66507092 0.59024823 0.6358156  0.64485816 0.64441489\n",
      " 0.6445922  0.65664894 0.61879433 0.65265957 0.5858156  0.65265957]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: RandomForestClassifier(bootstrap=False, min_samples_split=5, n_estimators=10)\n",
      "Mejor conjunto de parámetros: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10}\n",
      "Mejor puntuación de validación cruzada: 0.702659574468085\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100],  # Número de árboles en el bosque\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Número de características a considerar al buscar la mejor división\n",
    "    'max_depth': [None, 10, 20],  # Profundidad máxima del árbol\n",
    "    'min_samples_split': [2, 5],  # Número mínimo de muestras requeridas para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2],  # Número mínimo de muestras requeridas en cada hoja del árbol\n",
    "    'bootstrap': [True, False]  # Método para muestrear puntos de datos (con o sin reemplazo)\n",
    "}\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "forest_model = RandomForestClassifier()\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(forest_model, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "240 fits failed out of a total of 720.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.59414894 0.60239362 0.57278369 0.60230496\n",
      " 0.5856383  0.59796099 0.59414894 0.61498227 0.56923759 0.60664894\n",
      " 0.60239362 0.59796099 0.57703901 0.58953901 0.60230496 0.59796099\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.58147163 0.6231383  0.5481383  0.58971631\n",
      " 0.60274823 0.58572695 0.55611702 0.60221631 0.55664894 0.56897163\n",
      " 0.6108156  0.58528369 0.62349291 0.62322695 0.59804965 0.59379433\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.55673759 0.62703901 0.52322695 0.5983156\n",
      " 0.56046099 0.61072695 0.59840426 0.63590426 0.56870567 0.6108156\n",
      " 0.56462766 0.59388298 0.53927305 0.59397163 0.56489362 0.58572695\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.60647163 0.62774823 0.60656028 0.64796099\n",
      " 0.59432624 0.65656028 0.5981383  0.61462766 0.57349291 0.63998227\n",
      " 0.59822695 0.63164894 0.58980496 0.6231383  0.56914894 0.63191489\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.5643617  0.65656028 0.5733156  0.65265957\n",
      " 0.58546099 0.64849291 0.62765957 0.65673759 0.63191489 0.62739362\n",
      " 0.65248227 0.61897163 0.56072695 0.61054965 0.57367021 0.64414894\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.57739362 0.66090426 0.60212766 0.65700355\n",
      " 0.63164894 0.61480496 0.59840426 0.64822695 0.58111702 0.65265957\n",
      " 0.58590426 0.6606383  0.59406028 0.62340426 0.58164894 0.65638298]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: Pipeline(steps=[('pca', PCA(n_components=150)),\n",
      "                ('forest',\n",
      "                 RandomForestClassifier(bootstrap=False, max_depth=20))])\n",
      "Mejor conjunto de parámetros: {'forest__bootstrap': False, 'forest__max_depth': 20, 'forest__max_features': 'sqrt', 'forest__min_samples_leaf': 1, 'forest__min_samples_split': 2, 'forest__n_estimators': 100}\n",
      "Mejor puntuación de validación cruzada: 0.6609042553191491\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Definir el número de componentes principales para PCA\n",
    "n_components = 50\n",
    "\n",
    "# Crear un pipeline que incluya PCA y Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('forest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Parámetros para GridSearch\n",
    "param_grid = {\n",
    "    'forest__n_estimators': [10, 100],\n",
    "    'forest__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'forest__max_depth': [None, 10, 20],\n",
    "    'forest__min_samples_split': [2, 5],\n",
    "    'forest__min_samples_leaf': [1, 2],\n",
    "    'forest__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Mejor modelo: GaussianNB()\n",
      "Mejor conjunto de parámetros: {}\n",
      "Mejor puntuación de validación cruzada: 0.6109042553191489\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Crear el modelo de Naive Bayes\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Definir los parámetros para GridSearch (Naive Bayes no tiene muchos parámetros ajustables)\n",
    "param_grid = {\n",
    "    # No hay parámetros específicos de GaussianNB para ajustar con GridSearch\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(nb, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA + Crossvalidation + GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "4 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 471, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 408, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1303, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 454, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 514, in _fit\n",
      "    return self._fit_full(X, n_components)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 530, in _fit_full\n",
      "    raise ValueError(\n",
      "ValueError: n_components=192 must be between 0 and min(n_samples, n_features)=191 with svd_solver='full'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\alema\\Desktop\\paper\\nodulos_tiroideos\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.65265957 0.63998227        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: Pipeline(steps=[('pca', PCA(n_components=100)), ('naive_bayes', GaussianNB())])\n",
      "Mejor conjunto de parámetros: {'pca__n_components': 100}\n",
      "Mejor puntuación de validación cruzada: 0.6526595744680851\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ruta relativa a la carpeta que contiene las imágenes organizadas por etiquetas\n",
    "data_dir = \"db_unal/organized/images/cropped\"\n",
    "categories = ['high', 'low']\n",
    "\n",
    "# Listas para almacenar los datos de las imágenes y sus etiquetas\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Cargar las imágenes y las etiquetas desde las carpetas\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            # Convertir imagen a escala de grises y redimensionar\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            # Aplanar la imagen para crear un vector de características\n",
    "            flattened_image = image.flatten()\n",
    "            data.append(flattened_image)\n",
    "            labels.append(category)\n",
    "\n",
    "# Convertir las listas de datos y etiquetas en arrays de numpy\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Definir el número de componentes principales para PCA\n",
    "n_components = 50\n",
    "\n",
    "# Crear un pipeline que incluya PCA y Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('naive_bayes', GaussianNB())\n",
    "])\n",
    "\n",
    "# Parámetros para GridSearch (aunque Naive Bayes no tiene muchos parámetros ajustables)\n",
    "param_grid = {\n",
    "    'pca__n_components': [100, 150, 192]  # Ejemplos de posibles valores para n_components\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "\n",
    "# Ejecutar GridSearch\n",
    "grid_search.fit(data, labels)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "print(\"Mejor modelo:\", grid_search.best_estimator_)\n",
    "print(\"Mejor conjunto de parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación de validación cruzada:\", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
