{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Red neuronal convolucional para el diagnóstico de nódulos tiroideos según la clasificación EU-TIRADS**\n",
    "\n",
    "## Por Alejandro Martínez Hernández\n",
    "\n",
    "### Notebook 3/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creación de modelos**\n",
    "\n",
    "PAra la siguiente parte se crearan los siguientes modelos de clasificación simple:\n",
    "- model1\n",
    "- model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MobileNetV3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Small Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configurar generadores de imágenes con y sin aumento de datos\n",
    "def setup_image_generators(base_dir, input_size=(224, 224), batch_size=5):\n",
    "    \"\"\"\n",
    "    Configura generadores de imágenes para entrenamiento y validación.\n",
    "    \n",
    "    Args:\n",
    "    base_dir (str): Directorio base donde están ubicadas las carpetas de imágenes.\n",
    "    input_size (tuple): Tamaño al cual se redimensionarán las imágenes.\n",
    "    batch_size (int): Número de imágenes por lote.\n",
    "\n",
    "    Returns:\n",
    "    train_generator, validation_generator: Generadores para el entrenamiento y validación.\n",
    "    \"\"\"\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=5,\n",
    "        zoom_range=[0.5, 1.5],\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='constant',\n",
    "        validation_split=0.2  # Reserva el 20% de los datos para validación\n",
    "    )\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=input_size,\n",
    "        #batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'  # Especifica que este generador es para entrenamiento\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=input_size,\n",
    "        #batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'  # Especifica que este generador es para validación\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "def create_model(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal secuencial utilizando MobileNetV3 como base.\n",
    "    \n",
    "    Args:\n",
    "    input_shape (tuple): Dimensiones de las imágenes de entrada.\n",
    "    num_classes (int): Número de clases para la clasificación. Se espera 2 para binaria.\n",
    "\n",
    "    Returns:\n",
    "    model: Modelo compilado de TensorFlow.\n",
    "    \"\"\"\n",
    "    base_model = MobileNetV3Small(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    # Congelar el modelo base para reutilizar las características aprendidas en ImageNet\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Cambio para clasificación binaria\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Ruta al directorio de imágenes\n",
    "base_directory = 'db_unal/organized/images/cropped'\n",
    "train_gen, val_gen = setup_image_generators(base_directory)\n",
    "\n",
    "# Crear y compilar el modelo\n",
    "model = create_model()\n",
    "\n",
    "# Calcular los pesos de clase para manejar datos desbalanceados\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',  # Especifica 'balanced' para ajustar automáticamente en base a la frecuencia de clases\n",
    "    classes=np.unique(train_gen.classes),  # Obtener clases únicas de los datos de entrenamiento\n",
    "    y=train_gen.classes)  # Etiquetas de clase reales para cada muestra de entrenamiento\n",
    "\n",
    "# Crear un diccionario mapeando índices de clase a sus respectivos pesos calculados\n",
    "class_weights = {i: weights[i] for i in range(len(weights))}\n",
    "\n",
    "# Configurar el callback EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitorear la pérdida de validación\n",
    "    min_delta=0.01,      # Cambio mínimo detectado como una mejora\n",
    "    patience=50,          # Número de épocas sin mejora después de las cuales el entrenamiento será detenido\n",
    "    verbose=1,           # Mostrar mensajes de progreso\n",
    "    mode='min'           # 'min' porque queremos minimizar la pérdida\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=500,\n",
    "    class_weight=class_weights, # Ajuste de pesos para cada clase\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve accuracy results on training and validation data sets for each training epoch.\n",
    "acc = history.history['accuracy']  # Training accuracy\n",
    "val_acc = history.history['val_accuracy']  # Validation accuracy\n",
    "\n",
    "# Retrieve loss results on training and validation data sets for each training epoch.\n",
    "loss = history.history['loss']  # Training loss\n",
    "val_loss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "# Determine the number of epochs, based on the length of the accuracy list.\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "plt.plot(epochs, acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Accuracy')  # Label the y-axis as 'Accuracy'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation\n",
    "\n",
    "# Plot training and validation loss as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Create a new figure for loss visualization\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Loss')  # Label the y-axis as 'Loss'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SGD optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Definir generadores de imágenes con aumento de datos\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    #rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='constant',\n",
    "    validation_split=0.2  # Reservar 20% de los datos para validación\n",
    ")\n",
    "\n",
    "data_dir = 'db_unal/organized/images/raw'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    #batch_size=50,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Especificar que esto es para el conjunto de entrenamiento\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    #batch_size=20,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Especificar que esto es para el conjunto de validación\n",
    ")\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelar capas base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir nuevas capas\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=SGD(learning_rate=0.5, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calcular los steps per epoch y validation steps\n",
    "#train_steps = int(np.ceil(train_generator.samples / train_generator.batch_size))\n",
    "#val_steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    #steps_per_epoch=train_steps,\n",
    "    validation_data=validation_generator,\n",
    "    #validation_steps=val_steps,\n",
    "    epochs=25\n",
    ")\n",
    "\n",
    "# Retrieve accuracy results on training and validation data sets for each training epoch.\n",
    "acc = history.history['accuracy']  # Training accuracy\n",
    "val_acc = history.history['val_accuracy']  # Validation accuracy\n",
    "\n",
    "# Retrieve loss results on training and validation data sets for each training epoch.\n",
    "loss = history.history['loss']  # Training loss\n",
    "val_loss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "# Determine the number of epochs, based on the length of the accuracy list.\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "plt.plot(epochs, acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Accuracy')  # Label the y-axis as 'Accuracy'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation\n",
    "\n",
    "# Plot training and validation loss as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Create a new figure for loss visualization\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Loss')  # Label the y-axis as 'Loss'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Large Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SGD optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ResNet50**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SGD Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Definir generadores de imágenes con aumento de datos\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Reservar 20% de los datos para validación\n",
    ")\n",
    "\n",
    "data_dir = 'db_unal/organized/images/cropped'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Especificar que esto es para el conjunto de entrenamiento\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=6,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Especificar que esto es para el conjunto de validación\n",
    ")\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelar capas base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Añadir nuevas capas\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calcular los steps per epoch y validation steps\n",
    "train_steps = int(np.ceil(train_generator.samples / train_generator.batch_size))\n",
    "val_steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=25\n",
    ")\n",
    "\n",
    "# Retrieve accuracy results on training and validation data sets for each training epoch.\n",
    "acc = history.history['accuracy']  # Training accuracy\n",
    "val_acc = history.history['val_accuracy']  # Validation accuracy\n",
    "\n",
    "# Retrieve loss results on training and validation data sets for each training epoch.\n",
    "loss = history.history['loss']  # Training loss\n",
    "val_loss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "# Determine the number of epochs, based on the length of the accuracy list.\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "plt.plot(epochs, acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Accuracy')  # Label the y-axis as 'Accuracy'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation\n",
    "\n",
    "# Plot training and validation loss as a function of epochs.\n",
    "plt.figure(figsize=(10, 6))  # Create a new figure for loss visualization\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')  # Set the title of the graph\n",
    "plt.xlabel('Epochs')  # Label the x-axis as 'Epochs'\n",
    "plt.ylabel('Loss')  # Label the y-axis as 'Loss'\n",
    "plt.legend()  # Add a legend to distinguish training vs. validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ResNet101**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SGD Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VGG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG16**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Xception**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DenseNet**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
